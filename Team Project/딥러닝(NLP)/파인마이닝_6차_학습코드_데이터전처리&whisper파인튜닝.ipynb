{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50c77b9-5914-4bc6-b58d-030360259a97",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37741b37-af82-49b6-8d34-ff7f116f1b90",
   "metadata": {},
   "source": [
    "### 음성 데이터 (pcm -> wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919bf346-069a-417a-b72f-4d4dca48f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCM 파일을 WAV 파일로 변환 완료.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# 데이터 폴더와 결과 저장 위치 설정\n",
    "data_folder = 'C:/Users/MATH-1/dat_nlp/005.한영 혼합 인식 데이터/01.데이터/1.Training/원천데이터_0825_add'\n",
    "output_folder = 'converted_wav_files'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# PCM 파일의 샘플 레이트와 채널 수\n",
    "sample_rate = 16000  # 예시: 16kHz\n",
    "channels = 2         # 모노 음성\n",
    "\n",
    "# 폴더 내 모든 zip 파일 순회\n",
    "for zip_file in sorted(os.listdir(data_folder)):\n",
    "    zip_path = os.path.join(data_folder, zip_file)\n",
    "    \n",
    "    # zip 파일인지 확인 후 열기\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            # zip 파일 내 파일 리스트 탐색\n",
    "            for file in z.namelist():\n",
    "                if file.endswith('.pcm'):\n",
    "                    # 압축 해제된 PCM 파일 경로 설정\n",
    "                    extracted_pcm_path = z.extract(file, path=output_folder)\n",
    "                    \n",
    "                    # .wav 파일 경로 설정\n",
    "                    wav_file_path = os.path.join(output_folder, os.path.splitext(os.path.basename(file))[0] + '.wav')\n",
    "\n",
    "                    # PCM 파일을 WAV로 변환\n",
    "                    ffmpeg_command = [\n",
    "                        'ffmpeg', '-f', 's16le', '-ar', str(sample_rate), '-ac', str(channels),\n",
    "                        '-i', extracted_pcm_path, wav_file_path\n",
    "                    ]\n",
    "                    \n",
    "                    # FFmpeg 실행\n",
    "                    subprocess.run(ffmpeg_command)\n",
    "\n",
    "                    # 변환 완료 후 PCM 파일 삭제 (필요 시)\n",
    "                    os.remove(extracted_pcm_path)\n",
    "\n",
    "print(\"PCM 파일을 WAV 파일로 변환 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c2478-f3b6-4718-b271-d950c5d8ed24",
   "metadata": {},
   "source": [
    "### 라벨링 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08951c8c-cfbc-4ce0-abde-ebc3145eb686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          audio_path  \\\n",
      "0  C:/Users/MATH-1/dat_nlp/converted_wav_files\\전문...   \n",
      "1  C:/Users/MATH-1/dat_nlp/converted_wav_files\\전문...   \n",
      "2  C:/Users/MATH-1/dat_nlp/converted_wav_files\\전문...   \n",
      "3  C:/Users/MATH-1/dat_nlp/converted_wav_files\\전문...   \n",
      "4  C:/Users/MATH-1/dat_nlp/converted_wav_files\\전문...   \n",
      "\n",
      "                                   transcription  start_time  end_time  \n",
      "0       세로토닌 재흡수를 억제하면 우울이나 불안 symtom을 완화할 수 있어.       2.249    11.020  \n",
      "1      그럼 한올파모티딘정은 어떤 증상을 완화하는 데 쓰이는 medicine이야?      12.521    22.201  \n",
      "2  white 색의 원형 필름코팅정인데 위궤양이나 위식도 역류질환을 치료하는데 쓰여.      24.397    36.839  \n",
      "3  medicine 먹을 때 속 쓰리지 말라고 보통 20밀리그램 단위로 복용할 거야.      39.465    49.412  \n",
      "4            coronavirus메디슨 어떻해 할거야? 접종할거야? 말거야?      50.539    60.204  \n"
     ]
    }
   ],
   "source": [
    "'''import zipfile\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 폴더와 결과 저장 위치 설정\n",
    "data_folder = 'C:/Users/MATH-1/dat_nlp/005.한영 혼합 인식 데이터/01.데이터/1.Training/라벨링데이터_0825_add'\n",
    "wav_folder = 'C:/Users/MATH-1/dat_nlp/converted_wav_files'\n",
    "output_data = []\n",
    "\n",
    "# 폴더 내 모든 zip 파일 순회\n",
    "for zip_file in sorted(os.listdir(data_folder)):\n",
    "    zip_path = os.path.join(data_folder, zip_file)\n",
    "    \n",
    "    # zip 파일인지 확인 후 열기\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            # zip 파일 내 파일 리스트 탐색\n",
    "            for file in z.namelist():\n",
    "                if file.endswith('.json'):\n",
    "                    # JSON 파일 읽기\n",
    "                    with z.open(file) as f:\n",
    "                        data = json.load(f)\n",
    "                        dialogs = data.get(\"dialogs\", [])\n",
    "                        audio_file_name = file.replace(\".json\", \".wav\")  # 오디오 파일명 (wav로 수정)\n",
    "                        audio_path = os.path.join(wav_folder, audio_file_name)  # wav 파일 경로 설정\n",
    "\n",
    "                        for dialog in dialogs:\n",
    "                            # 삭제된 항목 무시\n",
    "                            if \"deleted\" not in dialog:\n",
    "                                text = dialog[\"text\"]\n",
    "                                # form -> originalForm으로 변환\n",
    "                                for expression in dialog.get(\"expression\", []):\n",
    "                                    text = text.replace(expression[\"form\"], expression[\"originalForm\"])\n",
    "\n",
    "                                # 데이터 저장\n",
    "                                output_data.append({\n",
    "                                    \"audio_path\": audio_path,\n",
    "                                    \"transcription\": text,\n",
    "                                    \"start_time\": float(dialog[\"startTime\"]),\n",
    "                                    \"end_time\": float(dialog[\"endTime\"])\n",
    "                                })\n",
    "\n",
    "# DataFrame으로 정리\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# 데이터 확인\n",
    "print(df.head())\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv(\"whisper_finetuning_data.csv\", index=False, encoding='utf-8-sig')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf107d2-4ee2-4320-9f25-d49e101606ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          audio_path  \\\n",
      "0  C:/Users/MATH-1/dat_nlp/converted_wav_files\\f1...   \n",
      "1  C:/Users/MATH-1/dat_nlp/converted_wav_files\\f1...   \n",
      "2  C:/Users/MATH-1/dat_nlp/converted_wav_files\\f1...   \n",
      "3  C:/Users/MATH-1/dat_nlp/converted_wav_files\\f1...   \n",
      "4  C:/Users/MATH-1/dat_nlp/converted_wav_files\\f1...   \n",
      "\n",
      "                                   transcription  start_time  end_time  \n",
      "0       세로토닌 재흡수를 억제하면 우울이나 불안 symtom을 완화할 수 있어.       2.249    11.020  \n",
      "1      그럼 한올파모티딘정은 어떤 증상을 완화하는 데 쓰이는 medicine이야?      12.521    22.201  \n",
      "2  white 색의 원형 필름코팅정인데 위궤양이나 위식도 역류질환을 치료하는데 쓰여.      24.397    36.839  \n",
      "3  medicine 먹을 때 속 쓰리지 말라고 보통 20밀리그램 단위로 복용할 거야.      39.465    49.412  \n",
      "4            coronavirus메디슨 어떻해 할거야? 접종할거야? 말거야?      50.539    60.204  \n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 폴더와 결과 저장 위치 설정\n",
    "data_folder = 'C:/Users/MATH-1/dat_nlp/005.한영 혼합 인식 데이터/01.데이터/1.Training/라벨링데이터_0825_add'\n",
    "wav_folder = 'C:/Users/MATH-1/dat_nlp/converted_wav_files'\n",
    "output_data = []\n",
    "\n",
    "# 폴더 내 모든 zip 파일 순회\n",
    "for zip_file in sorted(os.listdir(data_folder)):\n",
    "    zip_path = os.path.join(data_folder, zip_file)\n",
    "    \n",
    "    # zip 파일인지 확인 후 열기\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            # zip 파일 내 파일 리스트 탐색\n",
    "            for file in z.namelist():\n",
    "                if file.endswith('.json'):\n",
    "                    # JSON 파일 읽기\n",
    "                    with z.open(file) as f:\n",
    "                        data = json.load(f)\n",
    "                        dialogs = data.get(\"dialogs\", [])\n",
    "                        audio_file_name = os.path.basename(file).replace(\".json\", \".wav\")  # 파일명만 추출하여 .wav 확장자로 변경\n",
    "                        audio_path = os.path.join(wav_folder, audio_file_name)  # 최종 .wav 파일 경로 설정\n",
    "\n",
    "                        for dialog in dialogs:\n",
    "                            # 삭제된 항목 무시\n",
    "                            if \"deleted\" not in dialog:\n",
    "                                text = dialog[\"text\"]\n",
    "                                # form -> originalForm으로 변환\n",
    "                                for expression in dialog.get(\"expression\", []):\n",
    "                                    text = text.replace(expression[\"form\"], expression[\"originalForm\"])\n",
    "\n",
    "                                # 데이터 저장\n",
    "                                output_data.append({\n",
    "                                    \"audio_path\": audio_path,\n",
    "                                    \"transcription\": text,\n",
    "                                    \"start_time\": float(dialog[\"startTime\"]),\n",
    "                                    \"end_time\": float(dialog[\"endTime\"])\n",
    "                                })\n",
    "\n",
    "# DataFrame으로 정리\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# 데이터 확인\n",
    "print(df.head())\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv(\"whisper_finetuning_data.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2632afe0-ad11-4f36-b905-6f03c1f6106e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\math-1\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\math-1\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d4eec-0e54-46ab-ba06-a4bd6b4ec4da",
   "metadata": {},
   "source": [
    "## Whisper 파인튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43c672-5b2e-4157-9989-820e456f2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.3.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9470a48c-1588-437d-b97d-02732ac3f08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc93fee9-09f1-46b9-9b21-fdb2a4345ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad01ebbf-de0a-429e-8c2b-77bb09027c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\math-1\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.1 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.10.2.post1 pooch-1.8.2 soundfile-0.12.1 soxr-0.5.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a66cd9-23ca-4d31-a389-5b980c3be250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 전처리 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f3c93f2ff241c59794641185dfa4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/791883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# tqdm을 사용해 데이터셋 전처리\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터셋 전처리 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_data(x), remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 학습 파라미터 설정\u001b[39;00m\n\u001b[0;32m     30\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./whisper_finetuning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3428\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3426\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3428\u001b[0m     example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m   3429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3324\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# tqdm을 사용해 데이터셋 전처리\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터셋 전처리 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_data(x), remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 학습 파라미터 설정\u001b[39;00m\n\u001b[0;32m     30\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./whisper_finetuning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     38\u001b[0m )\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(examples):\n\u001b[1;32m---> 20\u001b[0m     audio \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(audio[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39maudio[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_features\n\u001b[0;32m     22\u001b[0m     labels \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mexamples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:377\u001b[0m, in \u001b[0;36mLazyRow.format\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_column(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_table\u001b[38;5;241m.\u001b[39mselect([key]))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:449\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_column(pa_table)\n\u001b[1;32m--> 449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:225\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_column\u001b[1;34m(self, column, column_name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_column(column, column_name) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\features\\features.py:2066\u001b[0m, in \u001b[0;36mFeatures.decode_column\u001b[1;34m(self, column, column_name)\u001b[0m\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode column with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m \n\u001b[0;32m   2056\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;124;03m        `list[Any]`\u001b[39;00m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 2066\u001b[0m         [decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   2067\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   2068\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[0;32m   2069\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\features\\features.py:1405\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image, Video)):\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode_example(obj, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\features\\audio.py:191\u001b[0m, in \u001b[0;36mAudio.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    189\u001b[0m array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmono:\n\u001b[1;32m--> 191\u001b[0m     array \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mto_mono(array)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate \u001b[38;5;241m!=\u001b[39m sampling_rate:\n\u001b[0;32m    193\u001b[0m     array \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mresample(array, orig_sr\u001b[38;5;241m=\u001b[39msampling_rate, target_sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\librosa\\core\\audio.py:508\u001b[0m, in \u001b[0;36mto_mono\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    505\u001b[0m util\u001b[38;5;241m.\u001b[39mvalid_audio(y, mono\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 508\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리된 데이터셋 로드 (예: CSV 파일로 저장한 경우)\n",
    "data = pd.read_csv(\"whisper_finetuning_data.csv\")  # 'audio_path', 'transcription' 열 포함\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.cast_column(r\"audio_path\", Audio())  # 오디오 파일 로드\n",
    "\n",
    "# Whisper 모델 및 프로세서 불러오기\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 데이터셋 전처리 함수\n",
    "def preprocess_data(examples):\n",
    "    audio = examples[\"audio_path\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    labels = processor(text=examples[\"transcription\"], return_tensors=\"pt\").input_ids\n",
    "    return {\"input_features\": inputs.squeeze(), \"labels\": labels.squeeze()}\n",
    "\n",
    "# tqdm을 사용해 데이터셋 전처리\n",
    "print(\"데이터셋 전처리 중...\")\n",
    "dataset = dataset.map(lambda x: preprocess_data(x), remove_columns=[\"audio_path\", \"transcription\"])\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper_finetuning\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,  # 여기서는 같은 데이터셋을 평가용으로 사용\n",
    ")\n",
    "\n",
    "# 파인튜닝 진행 (tqdm으로 진행 상황 표시)\n",
    "print(\"Whisper 모델 파인튜닝 시작...\")\n",
    "for epoch in tqdm(range(int(training_args.num_train_epochs)), desc=\"Epochs\"):\n",
    "    trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./whisper_finetuned_model\")\n",
    "print(\"모델 파인튜닝 완료 및 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da3c3a-3b73-4807-a9db-035c553f450e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 전처리 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ce47aaa1c147729c5c29dfc9a8f0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/791883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from tqdm import tqdm  # 기본 tqdm으로 변경\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리된 데이터셋 로드 (예: CSV 파일로 저장한 경우)\n",
    "data = pd.read_csv(\"whisper_finetuning_data.csv\")  # 'audio_path', 'transcription' 열 포함\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.cast_column(\"audio_path\", Audio())  # 오디오 파일 로드\n",
    "\n",
    "# Whisper 모델 및 프로세서 불러오기\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 데이터셋 전처리 함수\n",
    "def preprocess_data(examples):\n",
    "    audio = examples[\"audio_path\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    labels = processor(text=examples[\"transcription\"], return_tensors=\"pt\").input_ids\n",
    "    return {\"input_features\": inputs.squeeze(), \"labels\": labels.squeeze()}\n",
    "\n",
    "# tqdm을 사용해 데이터셋 전처리\n",
    "print(\"데이터셋 전처리 중...\")\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"audio_path\", \"transcription\"])\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper_finetuning\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,  # 여기서는 같은 데이터셋을 평가용으로 사용\n",
    ")\n",
    "\n",
    "# 파인튜닝 진행 및 진행 상황 표시\n",
    "print(\"Whisper 모델 파인튜닝 시작...\")\n",
    "trainer.train()  # trainer.train() 내에서 자체적으로 tqdm을 사용하여 진행 상황이 표시됩니다.\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./whisper_finetuned_model\")\n",
    "print(\"모델 파인튜닝 완료 및 저장 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
