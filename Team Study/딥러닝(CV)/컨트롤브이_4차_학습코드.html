<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>4주차 요약</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="0165bfe1-b9a1-4dad-b9ed-abdc3b0e1453" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">4️⃣</span></div><h1 class="page-title">4주차 요약</h1><p class="page-description"></p></header><div class="page-body"><h1 id="92c1ac8d-e61f-4f12-8fc4-774bab3a7baa" class=""><strong>파라미터 업데이트</strong></h1><p id="78cd7c1c-a31e-4cc2-96aa-275dbc950d45" class="">역전파로 계산된 분석적 기울기는 파라미터 업데이트를 수행한다. 심층 네트워크를 위한 최적화는 현재 매우 연구가 활발한 분야이다. 업데이트를 수행하는 몇몇의 접근법이 있다.</p><p id="c3c7ef8a-5650-43be-9693-5a3b25ab8173" class="">
</p><h3 id="5164e6d4-dbaa-49a3-8011-2eac332fb121" class="">SGD and bells and whistles</h3><p id="cae680f1-ac08-4f86-bb1c-680a975bf4e5" class=""><strong>Vanilla update</strong>.</p><p id="4ca60df7-6966-4608-bf7a-60fc4145558d" class="">간단한 업데이트 형태는 기울기의<strong> 음의 방향</strong>을 따라 파라미터를 바꾸는 것.</p><p id="3c098213-a207-400b-afd0-1274db7a1c1b" class="">기울기는 함수의 증가 방향을 나타내지만, 우리는 손실함수를 최소화해야 하므로 기울기의 음의 방향으로 이동해야 한다.</p><p id="ad1c34ac-6fa0-4e02-a18c-26f1555dbd12" class="">x →  파라미터로 구성된 벡터, dx → 손실함수의 기울기</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b5dc19b5-d521-4e7d-b9cd-7cf50f406560" class="code"><code class="language-JavaScript"># Vanilla update
x += - learning_rate * dx</code></pre><p id="8cf340d6-b16a-4af8-84a1-73a567d032e2" class="">여기서 learning_rate는 하이퍼파라미터로, 고정된 상수이다. 이 학습률이 충분히 낮으면, 전체 데이터셋에 대해 평가할 때, 손실함수가 항상 개선되도록 보장된다.</p><p id="beb3b8c1-1e85-423d-9e98-546f7b6cee81" class=""><br/>이 방식은 <br/><strong>기본 SGD</strong>(Vanilla SGD)라고도 불리며, 딥러닝 모델의 손실을 줄이는 가장 단순한 방법 중 하나입니다.</p><p id="a8573e93-bbeb-418d-8d6c-e9b55c62c642" class="">
</p><p id="c8789dae-d460-4104-8231-5aa222b5d817" class=""><strong>Momentum update 모멘텀 업데이트</strong></p><p id="ce58ba81-a257-49ec-917d-abe4b233c43b" class="">물리학에서 차용한 개념으로, 파라미터가 이전 업데이트 방향을 유지하면서 조금 더 가속도 있게 움직이도록 한다. 이를 통해 경사면에서 진동을 줄이고, 더 빠르게 최적화할 수 있다.</p><p id="d19cddb7-23c5-4af9-9f56-64b9e3c90e61" class=""><br/>* SGD는 <br/><strong>기울기의 방향</strong>을 따라 이동하는데, 손실 함수가 복잡한 <strong>곡면</strong>(예: 안장점 또는 긴 협곡 같은 구조)을 가지고 있으면, 기울기가 급격하게 변해 파라미터가 <strong>진동</strong>할 수 있다. 특히, <strong>경사 방향이 급격히 바뀌는</strong> 영역에서는 최적의 방향을 향해 나아가는 대신 <strong>좌우로 흔들리면서 느리게 수렴</strong>할 수 있다. </p><p id="044624e7-6bb4-450f-8c29-216524a60b0a" class="">→  모멘텀은 이전 단계의 이동 방향을 일정 비율로 유지하면서 새로운 기울기를 반영한다. 이를 통해 <strong>진동을 줄이고</strong> 모델이 더 일관된 방향으로 빠르게 수렴할 수 있게 만든다.</p><p id="aeee9980-b053-405b-bcbb-f11f832a310d" class=""><br/>* SGD는 <br/><strong>일관된 기울기</strong>를 따라 이동할 때에도 매번 기울기의 크기에만 의존해 이동하므로, <strong>수렴 속도가 느려질 수 있다</strong>. 특히, 좁고 긴 협곡 같은 최적화 지형에서는 느리게 이동하게 된다.<br/>→ 모멘텀은 <br/><strong>이전 기울기의 누적된 속도</strong>를 기반으로 이동한다. 즉, 일관된 방향의 기울기에서는 더 큰 속도를 쌓아 이동하므로, 수렴이 <strong>더 빠르게 이루어진다</strong>.<br/><br/><br/></p><p id="0627865f-0f9f-495b-a5bb-dd6d440d5048" class="">이전 기울기의 지수를 누적하여 현재 업데이트에 반영</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0ccb6252-3436-4983-9562-8d8cf4bc571b" class="code"><code class="language-JavaScript"># Momentum update
v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position</code></pre><p id="1f50325f-9ace-459e-a68f-e17b5734e560" class="">0으로 초기화된 속도변수 <strong>v </strong></p><p id="cb085db5-b877-48db-a8ec-508f6ab31fca" class="">추가적인 모멘텀 하이퍼파라미터 <strong>mu</strong> (보통 0.9로 설정)</p><p id="fdda5092-2179-430b-99f1-00510444bccc" class="">기울기의 속도를 감소시키는 역할을 함.</p><p id="2f65a02c-2936-4392-8d81-ac98e433d99b" class="">교차 검증 시 이 매개변수는 일반적으로 [0.5, 0.9, 0.95, 0.99]와 같은 값으로 설정됨. </p><p id="51078f1a-166b-4a8f-a964-9b00b7c1c916" class="">초기에는 0.5로 설정하고, 여러 에폭을 거쳐 0.99로 점진적으로 증가시키는 방식</p><p id="cc72e726-c785-4f60-86a8-3afc48be451d" class="">(확 증가시켰다가 천천히 증가시키는 방식)</p><p id="47e6f603-16a3-45c3-a9a5-e37bb272bad4" class="">
</p><p id="bc475337-6755-4d6d-877a-f18d70096227" class="">모멘텀 업데이트를 사용하면, <strong>일관된 기울기 방향</strong>으로 속도가 쌓여 파라미터가 더 빠르게 수렴할 수 있습니다.</p><p id="428727af-e1cb-4649-8292-648ace465d8e" class="">
</p><p id="499b6ace-0fe7-4379-9482-f87170ee1671" class=""><strong>Nesterov Momentum 네스테로프 모멘텀</strong></p><p id="a4bcb3ce-6f45-467d-a0b3-27a711dc022f" class="">모멘텀 기법의 변형으로, 현재 지점이 아닌 <strong>미리 예측된 지점</strong>에서 기울기를 계산하는 방식입니다. 이를 통해 더 정확한 기울기를 사용할 수 있게 되어, 학습 속도와 안정성이 개선될 수 있습니다.<br/><br/></p><p id="4a8c1190-324e-46ba-85e7-6bb0dda69dca" class="">현재 파라미터 벡터가 어떤 위치 x에 있을 때, 모멘텀 업데이트를 살펴보면 모멘텀 항만(기울기가 있는 두번째 항은 무시) 파라미터벡터를 mu*v만큼 밀어낼 것이라는 것을 알 수 있다. </p><p id="8a9df832-e286-4c66-b33e-3be3f85da9cd" class="">따라서 기울기를 계산할 때 미래의 대략적인 위치 x+mu*v를 미래예측으로 취급할 수 있다.</p><p id="3b297a16-4d20-46cf-ab33-b3f943f6e6cf" class="">이는 곧 도착할 지점 근처의 지점이다. 따라서 오래된 위치 x에서가 아니라 x+mu*v 위치에서 기울기를 계산하는 것이 합리적이다.</p><figure id="1c005a82-e850-47d2-8763-371a4f2b933a" class="image"><a href="image.png"><img style="width:678.1625366210938px" src="image.png"/></a></figure><p id="716f4947-746e-421a-b28b-e4bdf5a387f8" class="">현재 위치(빨간색 원)에서 기울기를 평가하는 대신, 모멘텀이 녹색화살표 끝까지 갈 것이라는 것을 알고 있으므로, Nesterov 모멘텀을 사용하면 ‘lookahead’위치에서 기울기를 평가하기 때문에 더 합리적인 결과를 낸다.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="0677e191-ffbc-4c4f-8f3a-ec53ba7c5e27" class="code"><code class="language-Python">x_ahead = x + mu * v
# evaluate dx_ahead (the gradient at x_ahead instead of at x)
v = mu * v - learning_rate * dx_ahead
x += v</code></pre><p id="67798332-938a-4eef-9955-8ae7d3c8f053" class="">x_ahead를 x로 업데이트 시킨 버전</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5d74251d-d962-431a-b697-08f48525199a" class="code"><code class="language-Python">v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += -mu * v_prev + (1 + mu) * v # position update changes form</code></pre><p id="1227be69-b54c-4e19-9d90-e2b4f0901690" class="">
</p><h3 id="c7a2d193-3632-489f-8a34-a49b81c40306" class=""><strong>Annealing the learning rate</strong></h3><p id="879dbf0d-a0d8-4e02-9341-228a3bf2cfd3" class="">심층 네트워크를 훈련할 때, 일반적으로 학습률을 시간이 경과함에 따라 어닐링하는 것이 도움이 된다.</p><p id="56a3e748-a17e-453b-a4f8-7719daf9db2d" class="">학습률이 높으면 시스템에 운동에너지가 너무많고 파라미터 벡터가 혼란스럽게 튀어나와 손실함수의 더 깊지만 좁은 부분에 정착할 수 없다.</p><p id="5568d7e0-51b8-4b81-b688-7cd0aa871aa9" class="">학습률을 너무 천천히 감소시키면 거의 개선되지 않고 시간이 너무 오래걸린다.</p><p id="0e129e10-1f07-4e9c-bc81-60639bf08611" class="">너무 공격적으로 감소시키면 시스템이 너무 빨리 냉각되어 최상의 위치에 도달할 수 없다.</p><p id="354d96b1-35fe-4cf0-8d15-b77e4a959b0f" class="">
</p><p id="92abd4d1-0b5a-4149-8e01-d36a2ce33bfd" class="">&lt;학습률 감소를 구현하는 3가지 유형&gt;</p><p id="ed19de83-8064-4949-88e4-cae7370c5c30" class="">
</p><ul id="07df6710-ddfe-4e06-a235-63fdec440ea9" class="bulleted-list"><li style="list-style-type:disc"><strong>Step decay 단계적 감소</strong>: 특정 에폭마다 학습률을 어느정도씩 줄인다.</li></ul><p id="72e26bee-f815-41fe-9e62-ac08cc95e634" class="">일반적으로 5에폭마다 학습률을 절반으로 줄이거나, 20에폭마다 0.1씩 줄인다.</p><p id="ef00ad12-c47c-40ef-9d62-d750ed0d5520" class="">실제로 볼 수 있는 방법은 고정된 학습률로 학습하는 동안 validation 오류를 살펴보고 더이상 개선되지 않을 때마다 학습률을 상수(예:0.5)만큼 줄이는 것이다.</p><p id="dedfe850-f50d-419c-b99c-f097d122c15e" class="">
</p><ul id="2868397b-910d-4b73-8bcc-901bcdc5581d" class="bulleted-list"><li style="list-style-type:disc"><strong>Exponential decay 지수적으로 감소: </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><msup><mi>e</mi><mrow><mo>−</mo><mi>k</mi><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\alpha=\alpha_0e^{-kt}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9991em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></li></ul><p id="f1b9e058-1f81-4440-9be2-e9175d71f140" class=""><strong> </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha_0, k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span><strong> 는 하이퍼파라미터, </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span><span>﻿</span></span><strong>는 반복수 (에폭 단위를 사용할 수도 있음)</strong></p><p id="a7261dc9-b9c7-4023-b87d-e21e70c57eb1" class="">
</p><ul id="64ebfe87-c7e3-4f73-a51e-5aa600905054" class="bulleted-list"><li style="list-style-type:disc"><strong>1/t decay: </strong><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>k</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha=\alpha_0/(1+kt)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span><span>﻿</span></span></li></ul><p id="81509e76-cda8-4e29-9dd1-87050fd17720" class=""><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha_0, k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span><span>﻿</span></span>는 하이퍼파라미터, <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span><span>﻿</span></span>는 반복수</p><p id="b07215c7-7fe1-48cb-bceb-78afdf0ee066" class="">
</p><p id="2150dc0a-4457-4a63-bd51-5849644a16e0" class="">감소하는 비율과 에폭단위의 타이밍 같은 Step decay의 하이퍼파라미터들이 해석하기 용이하기 때문에 Step decay가 선호된다.</p><p id="d0e0ee41-c83f-4fbc-becf-849ed8a7b83e" class="">계산에 드는 비용을 감당할 수 있다면 더 느린 decay를 택하고 더 오랜 시간동안 훈련하는 것이 좋다.</p><p id="13ca0407-57cd-45d7-aa6c-9d5145193171" class="">
</p><h3 id="7d77385f-9b8d-47c6-a6ec-f7ab335e2069" class=""><strong>Second order methods</strong></h3><p id="2994d3e6-4d0c-470a-a039-0dfe19e3427d" class=""><br/>신경망 최적화에서 <br/><strong>2차 미분</strong>인 **헤세 행렬(Hessian matrix)**을 사용하는 방법</p><p id="0ba39457-2d64-41b0-94c8-7501b7242f6b" class="">손실 함수의 곡률 정보(손실 함수의 기울기(1차 미분)가 변화하는 정도)를 이용하여 가중치를 업데이트하는 방식.</p><ul id="9b18d03d-0dd8-4b31-b14a-476768daa903" class="bulleted-list"><li style="list-style-type:disc"><strong>1차 미분(Gradient)</strong>: 손실 함수가 증가 또는 감소하는 방향을 나타냄.</li></ul><ul id="bfaa092c-3058-4cff-9c44-266c353d2391" class="bulleted-list"><li style="list-style-type:disc"><strong>2차 미분(Hessian matrix)</strong>: 손실 함수의 기울기(gradient)가 얼마나 빠르게 변하는지를 나타냄. 곡률 정보를 담고 있음.</li></ul><p id="8fcc3d4e-babb-4099-9bd1-f2dbe9ce4a8d" class="">이를 통해 최적화 과정에서 더 정교한 업데이트를 할 수 있습니다.<br/><br/></p><figure id="89d30ab4-bf7f-4392-8cde-33c9664cd1ce" class="image"><a href="image%201.png"><img style="width:382px" src="image%201.png"/></a></figure><p id="38ee4830-d092-4a9f-8408-e89ce766c0f9" class=""><br/><br/><strong>Hf(x)</strong>는 Hessian matrix, 함수의 2차 편미분의 정사각 행렬</p><p id="19ba71ac-9604-44b4-a0fa-46367c4d83c4" class=""><strong>∇f(x)</strong>는 경사하강법에서 볼 수 있는 기울기 벡터</p><p id="d685f8e3-8184-4d0f-bf51-20ca424f20e4" class="">업데이트 공식에 학습률 하이퍼파라미터가 없기 때문에 1차방법에 비해 이점이라고 할 수 있다.</p><p id="91b37453-09ad-4e99-82f0-0d1d0831aa54" class="">
</p><p id="35f51a6d-49be-423e-a5ca-dbfb0a545f18" class="">손실함수의 기울기뿐만 아니라, 곡률정보를 나타내는 헤세행렬의 역행렬을 곱해 파라미터를 업데이트하므로, 단순 기울기 기반 방법보다 더 정확한 방향으로 파라미터를 조정 가능하다.</p><p id="c7d1955b-7b80-479d-bacb-32ee85a2963b" class="">곡률이 얕은 방향(경사가 완만)으로는 큰 학습률을 사용하고, 곡률이 가파른 방향(경사가 급격)으로는 작은 학습률을 사용해 안정적인 학습이 가능하다.</p><p id="a134eafe-6f43-4b08-8eaf-903c623b242c" class="">
</p><p id="e67f87aa-b5bc-456e-8eca-32f2bedc84f6" class="">But! 헤세행렬을 계산하고 역행렬을 계산하는 것은 공간과 시간 측면에서 비용이 많이 들기 때문에 비실용적이다. </p><p id="ff429604-98ee-475f-bed8-1fe0c284761b" class="">현재 대규모 딥러닝 및 합성곱 신경망에서 2차방법은 거의 쓰이지 않고 모멘텀을 기반으로 하는 SGD 변형이 더 간단하고 확장하기도 쉽기 때문에 더 많이 쓰인다.</p><p id="8daed109-e3c2-4db5-8491-f24ce7f0d465" class="">
</p><h3 id="393bc0ae-a45a-457f-92ae-fb5e292a7188" class=""><strong>Per-parameter adaptive learning rate methods 파라미터별로 학습률을 다르게 조정</strong></h3><p id="3327d5fa-f9c6-4708-a176-466e9dd99c89" class="">위까지는 모든 파라미터에 대해 동일하게 학습률을 조정함.</p><p id="a37fc4a6-9173-4de1-8a38-2ce61fc27bcc" class="">
</p><p id="cb1a0405-d71e-4b9b-9576-a80119795011" class=""><strong>Adagrad: 파라미터별로 맞춤 학습률 적용</strong></p><p id="b39122ce-aa5e-45f7-b353-c649c767f77e" class="">자주 업데이트되는 파라미터(기울기가 큰)는 학습률을 줄이고, 드물게 업데이트되는 파라미터(기울기가 작은)는 학습률을 늘리는 방식. </p><p id="ccef9247-a2f6-4d8c-b220-6afb78098c3e" class="">학습의 균형을 맞추는 데 도움이 된다.</p><p id="fbad6393-18a9-4fbb-b414-649cd41d4f22" class="">학습이 진행될수록 학습률이 점차 감소하며, 더 안정적으로 수렴할 수 있게 한다.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="52e2376a-6d49-432d-aa7c-a32f4b0e6a8f" class="code"><code class="language-Python"># Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)</code></pre><p id="a72e9c7a-072f-48e5-8cc9-54be738f3ca3" class="">cache는 과거 기울기의 제곱합을 저장한 값</p><p id="50bd8a0e-a91e-4fc4-870d-1503fe676f46" class="">smoothing term eps(일반적으로 1e^(-4)에서 1e^(-8)사이의 어딘가에 설정됨)은 0으로 나눠지는 것을 방지. (분모가 0이 되지 않게 하는 역할)</p><p id="a67bfe6a-0e64-4b87-accb-4581e0390bfd" class="">파라미터 x에 대한 학습률이 시간이 지날수록 감소하게 되어, 자주 업데이트되는 파라미터는 점차 학습이 느려진다.</p><p id="e5404e52-9fc9-4bb7-9165-398df2f0818e" class="">
</p><p id="3e30ea33-8413-486c-b306-7d3ed9ab9f8a" class="">그러나 Adagrad는 학습률이 너무 빨리 감소하여, 일찍 학습을 중단할 수 있다는 단점이 있다.</p><p id="28931cc8-6d04-49be-851e-f2d4d0082517" class="">
</p><p id="f0884e14-e026-4ee5-b560-7d429385054c" class=""><strong>RMSprop</strong></p><p id="4a09a95c-5d91-47aa-ad1c-2377b1cf954c" class="">RMSProp는 Adagrad의 이러한 문제점을 해결하기 위해,</p><p id="485271f9-62a7-486c-9e61-945cb599b575" class="">기울기 제곱합(cache)의 <strong>이동 평균</strong>을 사용하여 과거 기울기의 영향을 부분적으로 반영.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d49476a7-8470-476c-a5fe-17491927135c" class="code"><code class="language-Python">cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)</code></pre><p id="d5aec459-ea26-4222-a77d-65d71a951030" class="">decay_rate는 과거 기울기 제곱합의 가중치를 조절하는 하이퍼파라미터이며, 일반적으로 [0.9, 0.99, 0.999] 이다.</p><p id="ca118de5-21f3-4cba-a635-24917b1fb190" class="">
</p><p id="d2ce4cd5-46d8-4f01-a9b1-d66fb300826d" class="">RMSProp은 여전히 그래디언트의 크기에 따라 각 가중치의 학습률을 조절하여 유익한 평준화 효과가 있지만 Adagrad와 달리 시간이 지나도 학습률이 너무 빠르게 감소하지 않는다.</p><p id="3196aa99-da84-4ebd-bea1-2188ab8b9989" class="">
</p><p id="2d19e674-d4df-4557-a283-d1b1bc9cc28c" class=""><strong>Adam : 최근 방법, 모멘텀 + RMSProp</strong></p><p id="18404761-37d5-4178-9589-7410b6a5d66e" class="">모멘텀을 추가하여 기울기의 방향성까지 고려한 최적화 알고리즘</p><p id="5c6f0337-2ba0-4a0e-b33a-dff99790fdec" class="">기울기의 1차 모멘트(평균)와 2차 모멘트(분산)를 동시에 추적하여 파라미터를 업데이트</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="79fe1fe8-de13-434d-868e-17c664bfffb8" class="code"><code class="language-Python">m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)</code></pre><p id="06bd5a73-63a9-4eda-ae54-5b4e074443de" class="">bias 보정 매커니즘 적용 후</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="352ba4b4-ffe0-4c72-baaa-9fff03033bd5" class="code"><code class="language-Python"># t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)</code></pre><p id="41c96223-918a-4109-8aed-3c53b03c37b2" class="">
</p><ul id="a83e670c-f9fc-47ec-bd78-b5beaf9afb99" class="bulleted-list"><li style="list-style-type:disc"><strong>빠른 수렴</strong>을 유도하며, <strong>모멘텀과 RMSprop의 장점을 모두 결합</strong>해 학습이 안정적</li></ul><ul id="70c1ebd9-f71b-4bf7-ae0b-b3288dad1aa9" class="bulleted-list"><li style="list-style-type:disc"><strong>넓은 범위의 문제</strong>에서 잘 작동하며, 기본적으로 <strong>튜닝이 덜 필요</strong>한 편</li></ul><p id="f492eee7-71e3-41c6-8b2d-de18af3cbcab" class="">
</p><h2 id="c2d37906-f03c-4384-b52b-cfe2a367c997" class=""><strong>Hyperparameter optimization 하이퍼파라미터 최적화</strong></h2><p id="1827bd16-1876-4447-9edc-c96b8ba394dc" class="">
</p><p id="c0a8144d-39a7-4ad5-9e23-f69b053f119b" class="">신경망에서 가장 일반적인 하이퍼파라미터 목록</p><p id="ec97e1b1-f029-4847-bd8d-9808976c7d14" class="">the initial learning rate 초기학습률<br/>learning rate decay schedule (such as the decay constant) 학습률 감소 일정(감소상수)<br/>regularization strength (L2 penalty, dropout strength) 정규화 강도(L2 패널티, 드롭아웃강도)<br/></p><p id="8949e8ce-2387-41b7-accb-34c380f2aa19" class="">그리고 모멘텀 설정 같은 상대적으로 덜 민감한 하이퍼파라미터들이 있다.</p><p id="3fa039c7-f4c1-4719-ad38-c8a0579ca28c" class="">
</p><p id="75dd2192-f57c-4fc6-b9e0-38e1092d44ce" class=""><strong>하이퍼파라미터 최적화에 관한 몇가지 팁</strong></p><p id="08b7dd40-3be4-4b79-965e-ef10f8215856" class="">
</p><ul id="84887190-c420-436f-8ea6-2d779cf94618" class="bulleted-list"><li style="list-style-type:disc"><strong>Implementation 구현</strong></li></ul><p id="2af3e60e-37e7-4c97-8f5f-305fd6f2a786" class="">대규모 신경망은 훈련 시간이 길기 때문에 하이퍼파라미터 검색은 며칠에서 몇 주까지 걸릴 수 있다.</p><p id="7b902818-97c4-44d9-91c3-6a0c336adaec" class="">이에 따라 코드를 효율적으로 작성하는 것이 중요하다. </p><p id="a5123b14-8c89-49d0-ab02-4621716a31b2" class="">일반적인 방식은 작업자(worker)가 무작위로 하이퍼파라미터를 샘플링하고 최적화를 수행하는 방식. 작업자는 매 에폭 후 검증 성능을 기록하고, 체크포인트를 저장함.</p><p id="628fef1a-6aba-4931-8c8b-8dee5616af0e" class="">이때 검증 성능을 파일 이름에 포함하면 진행 상황을 쉽게 파악할 수 있습니다. </p><p id="d69314e2-7837-48f5-b1e2-6b6be20f4866" class="">또 다른 프로그램인 마스터(master)가 여러 작업자를 관리하고, 체크포인트를 검사하며 학습 통계를 시각화하는 역할을 합니다.</p><p id="2a97aa2b-7504-4f79-96a7-92e2f3ad0f35" class="">
</p><ul id="d4c156e2-0b90-488d-9d84-60cc7fd05705" class="bulleted-list"><li style="list-style-type:disc"><strong>Prefer one validation fold to cross-validation 교차검증보다 단일 검증 세트 선호</strong></li></ul><p id="8cab7b1c-bb20-4c4d-ad38-7c07629b095a" class="">대부분의 경우 <strong>단일 검증 세트</strong>를 사용하는 것이 코드 구현을 단순하게 만든다. </p><p id="c179d3c1-4a53-464c-819f-d0199c620b24" class="">여러 폴드로 나누는 교차 검증보다 더 효율적이며, 적당한 크기의 검증 세트를 사용하는 것이 일반적임.</p><p id="c4c50618-15cf-4b21-91f1-83848fd5af0f" class="">
</p><ul id="418348ec-d352-4daa-85e6-c6b55b8a57b5" class="bulleted-list"><li style="list-style-type:disc"><strong>Hyperparameter ranges 하이퍼파라미터 범위 설정</strong></li></ul><p id="4fd40d17-8397-4cd7-b056-8d829a422115" class="">하이퍼파라미터는 <strong>로그 스케일</strong>에서 검색하는 것이 좋다. (로그 범위에서 무작위로 샘플링하는 것을 말함)</p><p id="14e9ac33-3787-4ca5-87d8-6d1e1c1d36e9" class="">예를 들어, 학습률이 <code>learning_rate = 10 ** uniform(-6, 1)</code>처럼 균일분포에서 무작위로 값을 선택한다고 했을 때, 10의 거듭제곱 형태로 값을 선택한다. 이렇게하면 작은 값부터 큰 값까지 비율적으로 고르게 분포된 값을 찾을 수 있다.</p><p id="08387e6b-8d3e-4869-b4e7-b158111e50bb" class="">이는 학습률이나 정규화 강도 같은 하이퍼파라미터들이 곱셈적인 효과를 가지기 때문입니다. </p><p id="aaedee88-afad-4520-b3b3-10aa955cd7cd" class="">반면, 드롭아웃과 같은 일부 파라미터는 원래 범위에서 탐색할 수 있습니다. (<strong>드롭아웃 비율</strong>은 0과 1 사이에서 선형적으로 분포하는 값이기 때문에 이 경우에는 값이 곱셈적이지 않고, 단순히 0에서 1 사이에서 선형적으로 고르게 분포된 값을 선택하는 것이 적절)</p><p id="856f8d7e-09e5-4943-8d9a-656b301083a1" class="">
</p><ul id="434fd9a0-e020-4f32-9c1e-e4e01de7388e" class="bulleted-list"><li style="list-style-type:disc"><strong>Prefer random search to grid search 랜덤 서치를 그리드서치보다 선호</strong></li></ul><p id="7f9cc24b-6b35-4cb0-9913-7a653db5a740" class="">랜덤으로 선택된 시행은 그리드에서 선택된 시행보다 하이퍼파라미터 최적화에 더 효율적. 구현하기 쉽다.</p><figure id="c3abe71c-109d-4f39-b276-7338bdadde6d" class="image"><a href="image%202.png"><img style="width:678.1875px" src="image%202.png"/></a></figure><p id="3bac0902-2a36-448c-88a5-62316306965a" class="">
</p><p id="4453b8c0-bfd3-4825-aa6d-15fdce7b1122" class="">
</p><ul id="3fbdce25-f2bc-4645-8b9e-35035dea1eeb" class="bulleted-list"><li style="list-style-type:disc"><strong>Careful with best values on border 최적 값이 범위 경계에 있는지 확인</strong></li></ul><p id="4a9f9e5a-227e-4915-bd3e-114141f11a04" class="">하이퍼파라미터를 탐색할 때, 예를 들어 학습률을 <code>10 ** uniform(-6, 1)</code> 범위에서 찾는다면, 최종 결과가 <strong>탐색 범위의 경계에 위치</strong>하는지 확인해야 한다. </p><p id="945e2a82-a8ce-4258-ba5e-e2d0fd71c19b" class="">만약 최적의 값이 경계에 있다면, 그 범위 너머에 더 나은 값이 있을 가능성이 있으므로 <strong>탐색 범위를 확장</strong>할 필요가 있다.</p><p id="8789ca7c-78cb-4b06-8e92-a309ebe4cd83" class="">
</p><ul id="e5ad2f5f-dcf0-4b21-b25f-3797f5b86d2a" class="bulleted-list"><li style="list-style-type:disc"><strong>Stage your search from coarse to fine 단계를 나누어 탐색 (거친 탐색에서 세밀한 탐색으로)</strong></li></ul><p id="7147cbc9-a58c-44d1-9ba5-3e0c2609973a" class="">하이퍼파라미터 탐색을 할 때, 처음에는 넓은 범위에서 <strong>거친 탐색</strong>을 하고, 이후 좋은 값들이 발견되는 범위를 중심으로 <strong>세밀한 탐색</strong>을 진행하는 것이 좋다. </p><p id="9a5fef33-fddb-433b-a35a-11a01b5cbb77" class="">예를 들어, 먼저 10 ** [-6, 1] 범위에서 1 에포크만 학습하여 모델이 전혀 학습하지 않거나, 비용이 무한대로 증가하는 경우를 걸러낸다. 이후 더 좁은 범위에서 5 에포크로 탐색하고, 마지막으로 최종 범위에서 많은 에포크로 <strong>상세한 탐색</strong>을 수행할 수 있다.</p><p id="794ce9bd-c75f-40a3-900a-bee305db5851" class="">
</p><ul id="6865a930-d114-42b5-9f07-638e7fca77e8" class="bulleted-list"><li style="list-style-type:disc"><strong>Bayesian Hyperparameter Optimization 베이지안 하이퍼파라미터 최적화</strong></li></ul><p id="3b5f4af5-4eb1-4ebd-8c29-2dd8259c7889" class=""><strong>베이지안 하이퍼파라미터 최적화</strong>는 하이퍼파라미터 탐색을 더 효율적으로 수행하기 위한 연구 분야입니다. 이 방법은 <strong>탐색과 활용의 균형</strong>을 맞춰, 서로 다른 하이퍼파라미터에서의 성능을 더 효과적으로 조사하는 알고리즘을 개발한다. Spearmint, SMAC, Hyperopt와 같은 라이브러리들이 이 모델을 기반으로 개발되었다.</p><p id="8beab75f-b847-46d6-b0f8-f5591334b552" class="">하지만 실제로는 <strong>랜덤 검색</strong>이 신중하게 선택된 범위에서 <strong>베이지안 최적화보다 더 나은 성과</strong>를 보이는 경우도 많습니다. 특히 ConvNets 같은 복잡한 모델에서는 랜덤 검색이 여전히 효과적이다.</p><p id="f891be55-5624-41ae-ad08-6fdcf1a91eaa" class="">
</p><h3 id="91f52a9e-5536-44fb-af82-e907f023c6f7" class=""><strong>Evaluation 평가</strong></h3><h3 id="839552e0-4b17-4271-a1cd-b68bfe9b8501" class="">Model Ensembles 모델 앙상블</h3><p id="93c2081e-9f36-481a-b361-9741b8e3b66d" class="">실제로 신경망의 성능을 개선하는 신뢰할 수 있는 방법 중 하나는 여러 개의 독립 모델을 학습시키고 테스트 시 예측을 평균내는 방식으로 성능을 개선하는 방법이다.</p><p id="0f01a90c-f842-46b3-af63-7159fd3a0e1c" class="">앙상블의 모델 수가 증가함에 따라 성능은 일반적으로 단조롭게 개선된다(수익은 감소..). 게다가 앙상블의 모델 다양성이 높을수록 개선이 더 극적이다.</p><p id="a8102ed3-b4d7-4d91-b79d-4f183ab696b3" class="">
</p><p id="a3c2fbcd-9867-41ae-ae4e-4c38eecdb5e8" class=""><strong>앙상블을 형성하는 방법</strong></p><p id="f3a0b450-7113-4c18-a60f-0d720b838216" class="">
</p><p id="2bb4f6a0-3915-4fb6-8dfb-3efe70062851" class=""><strong>Same model, different initializations 같은 모델, 다른 초기화</strong></p><p id="852b03b2-8ff2-461f-926f-70656df2c196" class=""> 동일한 하이퍼파라미터로 여러 모델을 다른 랜덤 초기화 값으로 학습시킨다. 이 방법은 초기화에 의한 차이로만 다양성을 확보하기 때문에 한계가 있을 수 있다.</p><p id="5abe57ad-99f3-47d6-8656-c18effb8d664" class=""><strong>Top models discovered during cross-validation 교차검증 동안 발견된 상위 모델들</strong></p><p id="4e0af4c3-f63d-45f4-b6f7-1c049540458a" class="">교차 검증에서 상위 몇 개의 모델(예: 10개)을 선택해 앙상블을 구성. 이 방법은 더 다양한 모델을 포함할 수 있지만, 성능이 덜 최적화된 모델이 포함될 위험이 있다. 추가적인 학습이 필요하지 않아 구현이 상대적으로 쉽다.</p><p id="1d4505bd-3ad3-49b9-aa1f-5cb19b065d9b" class=""><strong>Different checkpoints of a single model 하나의 모델의 다른 체크포인트 사용</strong></p><p id="06250db5-29d5-4eaa-b9c9-be32fcb30e65" class="">학습이 매우 비용이 많이 드는 경우, 에폭마다 저장된 모델 체크포인트를 사용해 앙상블을 구성할 수 있다. 다양성은 떨어지지만, 실전에서는 적당히 효과적이며 비용이 적게 든다.</p><p id="6edc61b5-7d81-405c-884e-bd588271a472" class=""><strong>Running average of parameters during training 훈련 중 파라미터의 평균 사용</strong></p><p id="22f9982f-d4ca-47c8-aff6-ce6af6bca752" class="">학습 중 네트워크의 가중치들을 기억하고, 이전 가중치들의 지수적으로 감쇠된 합을 유지하는 두 번째 네트워크 가중치를 사용하는 방법. 이를 통해 네트워크 상태의 평균을 구하여 더 나은 검증 오류를 얻을 수 있다.</p><p id="7655c3bc-f1bd-4fea-8246-fad5dd01d501" class=""><br/>모델 앙상블의 단점 중 하나는 테스트 시 시간이 더 오래 걸린다는 것이다. 이를 해결하기 위한 연구로는 <br/><strong>Geoff Hinton</strong>의 “<strong>Dark Knowledge</strong>”가 있는데, 앙상블의 결과를 단일 모델로 &quot;증류(distill)&quot;하는 방법이 제안한다. 이 방법은 앙상블의 로그 가능도를 수정된 목표 함수에 통합하여 성능을 개선한다.<br/><br/><br/></p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>