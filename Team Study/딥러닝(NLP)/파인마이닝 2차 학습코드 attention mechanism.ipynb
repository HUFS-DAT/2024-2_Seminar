{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd60e88b-4fd2-4490-9cfe-74ce76f36733",
      "metadata": {
        "id": "fd60e88b-4fd2-4490-9cfe-74ce76f36733"
      },
      "source": [
        "# Attention Mechanism\n",
        "디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다.\n",
        "\n",
        "\n",
        " ![image.png](attachment:ca5142bc-f2ae-4072-9724-8a2c2ba5366e.png)\n",
        "\n",
        "### Attention 함수\n",
        ">- Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n",
        ">- K = Keys : 모든 시점의 인코더 셀의 은닉 상태\n",
        ">- V = Values : 모든 시점의 인코더 셀의 은닉 상태\n",
        "\n",
        "### Dot-Product Attention\n",
        "\n",
        "\n",
        "1. Attention score((디코더의 현재 상태와 인코더의 은닉 상태들에 대해 dot product)를 구한다. => Attention score 모음값![image.png](attachment:2c4d68df-00c8-4f85-b926-655ad8342c90.png)를 구한다. => 이에 대해 softmax를 적용하여 어텐션 분포(어텐션 가중치에 대한)를 구한다. => 어텐션 가중치와 인코더의 은닉 상태를 가중합한다. => 디코더의 현재 상태와 결합(concatenation)하여 ![image.png](attachment:24ed93fb-75e2-40cd-b1cb-a4fa3e3be082.png)를 구한다. => vt를 가중치 행렬과 곱한 후 tanh함수를 적용하여 ![image.png](attachment:06ba397d-823c-4c83-9698-97d98fbaaec2.png)를 얻는다. 출력층의 입력으로 이를 사용한다.\n",
        "\n",
        "\n",
        "![image.png](attachment:42257e74-b476-4ee0-bdf2-fb33c6e2c20f.png)\n",
        "\n",
        "![image.png](attachment:f2d9c325-1dfa-41d9-ac6f-ade4fb204146.png)\n",
        "\n",
        "![image.png](attachment:784ad924-d27b-428e-a752-8fbf8f810e51.png)\n",
        "\n",
        "![image.png](attachment:158a5617-7cfc-4124-9aa3-cda0c7409d4c.png)\n",
        "\n",
        "![image.png](attachment:3d7adb0e-ed76-43af-87bc-cd33eb5d6ca8.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eea98641-0abd-4fa6-9144-238c5f4bdbbd",
      "metadata": {
        "id": "eea98641-0abd-4fa6-9144-238c5f4bdbbd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9bed942-71fb-498e-b921-19a32473fbd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9bed942-71fb-498e-b921-19a32473fbd3",
        "outputId": "9f69e487-75ee-4cde-80aa-e9d6177e890b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cfc4979a-ac08-4144-a497-0a66b603f89e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfc4979a-ac08-4144-a497-0a66b603f89e",
        "outputId": "bdfe7d92-4d30-49fc-9cb8-3b35f2449aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰의 최대 길이 : 2494\n",
            "리뷰의 평균 길이 : 238.71364\n"
          ]
        }
      ],
      "source": [
        "print('리뷰의 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
        "print('리뷰의 평균 길이 : {}'.format(sum(map(len, X_train))/len(X_train)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cb99d695-adde-4435-b351-c628cd4c32dc",
      "metadata": {
        "id": "cb99d695-adde-4435-b351-c628cd4c32dc"
      },
      "outputs": [],
      "source": [
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f86f3aeb-1c4d-4b3f-ac67-d6398bd2e624",
      "metadata": {
        "id": "f86f3aeb-1c4d-4b3f-ac67-d6398bd2e624"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    # values에 적용되는 가중치\n",
        "    self.W2 = Dense(units)\n",
        "    # query에 적용되는 가중치\n",
        "    self.V = Dense(1)\n",
        "    # 스코어를 계산하는 가중치\n",
        "\n",
        "  def call(self, values, query): # 단, key와 value는 같음\n",
        "    # query shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7ea9cfb1-6992-49cc-9e5f-83463057b02f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ea9cfb1-6992-49cc-9e5f-83463057b02f",
        "outputId": "6534deef-0c0b-474e-f700-5b3efd933dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'bahdanau_attention' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1085s\u001b[0m 11s/step - accuracy: 0.6280 - loss: 0.6174 - val_accuracy: 0.8674 - val_loss: 0.3142\n",
            "Epoch 2/3\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1122s\u001b[0m 11s/step - accuracy: 0.9002 - loss: 0.2666 - val_accuracy: 0.8848 - val_loss: 0.2808\n",
            "Epoch 3/3\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1081s\u001b[0m 11s/step - accuracy: 0.9304 - loss: 0.2014 - val_accuracy: 0.8872 - val_loss: 0.2752\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "\n",
        "\n",
        "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
        "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)\n",
        "\n",
        "\n",
        "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)\n",
        "\n",
        "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \\\n",
        "  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n",
        "\n",
        "\n",
        "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
        "state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
        "\n",
        "attention = BahdanauAttention(64) # 가중치 크기 정의\n",
        "context_vector, attention_weights = attention(lstm, state_h)\n",
        "\n",
        "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzgC6Giu0akL",
        "outputId": "d3e4ade5-a846-4974-93df-b93c36d3f3f1"
      },
      "id": "pzgC6Giu0akL",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 382ms/step - accuracy: 0.8868 - loss: 0.2770\n",
            "\n",
            " 테스트 정확도: 0.8872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKhoVqjQUC7Y"
      },
      "id": "kKhoVqjQUC7Y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}